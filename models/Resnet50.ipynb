{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet50.ipynb",
      "provenance": [],
      "background_execution": "on",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50 - Colab notebook\n",
        "This notebook trains the Resnet50 network on either the ISIC dataset or the NIH Chest X-ray dataset (depending which cell is executed). Note that the adversarial configurations do not execute successfully, but they are kept in the codebase to document the original intention of the project."
      ],
      "metadata": {
        "id": "dAY1CYPGIw3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation and mount drive\n",
        "!pip install timm==0.4.12 yacs==0.1.8 adversarial-robustness-toolbox\n",
        "!pip install -U PyYAML\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H7qiiA1k5axd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnet50 Pytorch"
      ],
      "metadata": {
        "id": "qN0Lbjvm5DnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code adapted with modifications from https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html \n",
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import sklearn\n",
        "from timm.utils import accuracy, AverageMeter\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode\n"
      ],
      "metadata": {
        "id": "6EdnKsLF49oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove hidden ipynb checkpoints and .DS_Store from data folders to avoid later FileNotFoundError\n",
        "!rm -R /content/drive/MyDrive/MLSP_Masters/ECE_697/data/isic/isic_org/.ipynb_checkpoints\n",
        "!rm -R /content/drive/MyDrive/MLSP_Masters/ECE_697/data/isic/isic_org/.DS_Store"
      ],
      "metadata": {
        "id": "d-fYYvG3899t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute cell to use ISIC dataset\n",
        "data_dir = '/content/drive/MyDrive/MLSP_Masters/ECE_697/data/isic/isic_org' \n"
      ],
      "metadata": {
        "id": "Roj687KydHFb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute cell to use NIH Chest x-ray dataset\n",
        "data_dir = '/content/drive/MyDrive/MLSP_Masters/ECE_697/data/chxray'"
      ],
      "metadata": {
        "id": "Dv0bAHRyhCMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Train on ISIC Dataset\n",
        "data_dir = '/content/drive/MyDrive/MLSP_Masters/ECE_697/data/isic/isic_org'\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                  data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, # batch size is small due to memory constraints\n",
        "              shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# AUC calculation \n",
        "from sklearn.metrics import roc_auc_score\n",
        "def cal_auc(outputs, targets):\n",
        "    outputs = outputs.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "\n",
        "    if outputs.ndim == 1:\n",
        "        # try-except block to avoid incorrect ValueError\n",
        "        try:\n",
        "            auc = roc_auc_score(targets, outputs)\n",
        "        except ValueError:\n",
        "            pass\n",
        "    else:\n",
        "        n_classes = outputs.shape[1]\n",
        "        auc = 0\n",
        "        for i in range(n_classes):\n",
        "            try:\n",
        "                label_auc = roc_auc_score(targets==i, outputs[:,i])\n",
        "            except ValueError:\n",
        "                pass\n",
        "            auc += label_auc\n",
        "        auc /= n_classes\n",
        "    return auc"
      ],
      "metadata": {
        "id": "kiWOLpXv5Jpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=200):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    all_output, all_target = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    #del outputs\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                acc1, acc5 = accuracy(outputs,labels,topk=(1,5))\n",
        "\n",
        "                all_output.append(outputs.detach().cpu().numpy())\n",
        "                all_target.append(labels.detach().cpu().numpy())\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            time_elapsed = time.time() - since\n",
        "            print(\"This epoch takes \", time_elapsed)\n",
        "\n",
        "            all_output = np.concatenate(all_output)\n",
        "            all_target = np.concatenate(all_target)\n",
        "            auc = cal_auc(all_output, all_target)\n",
        "            print(f\"* AUC: {auc:.5f}\")\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "5FxrY-gQ7QDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.resnet50(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "W_0h0KWZ7S2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=200)"
      ],
      "metadata": {
        "id": "_lcp_-eR7iCj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}